{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bdb1fd0f-444a-46e8-a85b-caffe9370de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRYING TO MAKE BIDIRECTIONAL\n",
    "#ISSUE WITH HIDDEN SIZE SHAPE NOT SURE ABOUT THIS ADDITION STUFF\n",
    "#MAYBE FIND CODE FRO A BIDIRECTIONAL LSTM AS INSPO\n",
    "#GOTO START HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98e8784e-773c-4fef-bc5d-1cddaf6dcb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_class import SleepDataset, SleepChunkDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d5b831c-0b5d-4cc6-97e9-baff5d9b0115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import math\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score,precision_score, recall_score, f1_score\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torchmetrics.classification import MulticlassCohenKappa\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "from dataset_class import SleepDataset, SleepChunkDataset\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2b95e6-8936-479f-adda-39e37f72f66a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ActiNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0ebcd36-1c9d-4ca0-b405-32e964846c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DeepACTINeT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepACTINeT, self).__init__()\n",
    "        # Define the layers as per the structure provided\n",
    "        self.conv1 = nn.Conv1d(in_channels=3, out_channels=16, kernel_size=512, stride=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=16, kernel_size=256, stride=2)\n",
    "        self.conv3 = nn.Conv1d(in_channels=16, out_channels=16, kernel_size=256, stride=2)\n",
    "        self.conv4 = nn.Conv1d(in_channels=16, out_channels=16, kernel_size=32, stride=2)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(16)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(16)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(16)\n",
    "        self.batch_norm4 = nn.BatchNorm1d(16)\n",
    "        \n",
    "        # self.fc1 = nn.Linear(16, 64)  # Adjust input size based on final pooled size\n",
    "        # self.fc2 = nn.Linear(64, 10)  # Replace 10 with the number of output classes\n",
    "    \n",
    "    def forward(self, x):\n",
    "        assert not torch.isnan(x).any(), \"NaN detected in CNN input\"\n",
    "        x = x.permute(0,2, 1) #flip channels and num samples [batch, ch, num samples]\n",
    "\n",
    "        initial_samples = x.shape[2]\n",
    "        #print(initial_samples)\n",
    "        #you want to end up with one every 5 seconds \n",
    "        final_samples = initial_samples // (32*5)\n",
    "        print(final_samples)\n",
    "\n",
    "        \n",
    "        print(f'start shape {x.shape}')\n",
    "        x = F.relu(self.conv1(x))\n",
    "        print(f\"Input shape after conv1: {x.shape}\")\n",
    "        x = self.dropout(x)\n",
    "        print(f\"Input shape after, dropout: {x.shape}\")\n",
    "        x = self.batch_norm1(x)\n",
    "        print(f\"Input shape after conv1,dropout,batchnorm: {x.shape}\")\n",
    "        x = F.max_pool1d(x, kernel_size=2)\n",
    "        print(f\"Input shape after maxpool: {x.shape}\")\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        print(f\"Input shape after conv2: {x.shape}\")\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        print(f\"Input shape after conv3: {x.shape}\")\n",
    "        \n",
    "        x = F.max_pool1d(x, kernel_size=2)\n",
    "        print(f\"Input shape after maxpool: {x.shape}\")\n",
    "        \n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch_norm4(x)\n",
    "        print(f\"before avg pooling shape: {x.shape}\")\n",
    "\n",
    "        x = F.adaptive_avg_pool1d(x, final_samples)\n",
    "        print(f\"Final output shape: {x.shape}\")\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa53743-80de-4e2e-b461-e51b3337a29f",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f9ddc0c-c954-4046-a055-70652ec06f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/scratch/npr264/BioDeepL/dreamt/physionet.org/files/dreamt/2.0.0/data_64Hz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1a2baaf-ac56-4ee9-895f-5d3be759796a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SleepStager(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                cnn_output_channels: int = 16,\n",
    "                pre_lstm_embedding_size: int = 16,\n",
    "                lstm_hidden_size:   int = 128,\n",
    "                lstm_layers:        int = 6,\n",
    "                hidden_size2:       int = 64,\n",
    "                num_sleep_stages:   int = 5,\n",
    "                lr:                  float = 1e-3,\n",
    "                weight_tensor:      torch.Tensor = None,\n",
    "                debug:            bool = False):\n",
    "        super().__init__()\n",
    "        # self.save_hyperparameters()\n",
    "        \n",
    "        self.acc_cnn = DeepACTINeT()\n",
    "        #add non linearity to the bvp and temp before lstm \n",
    "        self.embeddingLayer = nn.Linear(2, pre_lstm_embedding_size)\n",
    "    \n",
    "        self.lstm_input_size = pre_lstm_embedding_size + cnn_output_channels\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.lstm = nn.LSTM(input_size=self.lstm_input_size,\n",
    "                            hidden_size=lstm_hidden_size,\n",
    "                            bias = True,\n",
    "                            batch_first=True,\n",
    "                            bidirectional = True)\n",
    "        #From paper \n",
    "        self.hidden2 = nn.Linear(lstm_hidden_size*2, hidden_size2)\n",
    "        self.classifier = nn.Linear(hidden_size2, num_sleep_stages)\n",
    "    \n",
    "        self.criterion = nn.CrossEntropyLoss(weight=weight_tensor, ignore_index=-1)\n",
    "        self.lr        = lr\n",
    "        self.debug      = debug\n",
    "        self.kappa = MulticlassCohenKappa(num_classes=num_sleep_stages)\n",
    "\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "        # Can pass on the entirety of lstm_out to the next layer if it is a seq2seq prediction\n",
    "        #self.outputLayer = nn.Linear(self.hidden_size*self.seq_len, self.output_size)\n",
    "\n",
    "    def forward(self, acc, non_acc):\n",
    "        \"\"\"\n",
    "        non_acc: (batch, non_acc_length, non_acc_dim)\n",
    "        acc:     (batch, acc_length)  or  (batch, acc_length, 1)\n",
    "        returns:\n",
    "          y_hat (output_length, batch, num_sleep_stages)\n",
    "        \"\"\"\n",
    "        # ensure ACC has a channel dim\n",
    "        if acc.dim() == 2:\n",
    "            acc = acc.unsqueeze(-1)          # now (batch, T, 1)\n",
    "        \n",
    "        # 1) ACC → CNN → (batch, num_cnn_feats, cnn_output_length)\n",
    "        acc_feats = self.acc_cnn(acc)\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] ACC CNN output shape: {acc_feats.shape}\") # (batch, num_cnn_feats, cnn_output_length)\n",
    "            print(f\"[DEBUG] NON_ACC input shape: {non_acc.shape}\") # (batch, non_acc_length, non_acc_dim)\n",
    "\n",
    "        #embed the non_acc data \n",
    "        emb_non_acc = self.embeddingLayer(non_acc)\n",
    "        emb_non_acc = self.dropout(emb_non_acc)\n",
    "        # 3) build LSTM input: (T', batch, feature_dim)\n",
    "        a = acc_feats.permute(2, 0, 1)        # (lstm_seq_len, batch, cnn_output_features)\n",
    "        b = emb_non_acc.permute(1, 0, 2)       # (lstm_seq_len, batch, non_acc_dim)\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] ACC CNN reshaped shape: {a.shape}\") # (batch, num_cnn_feats, cnn_output_length)\n",
    "            print(f\"[DEBUG] NON_ACC reshaped shape: {b.shape}\") # (batch, non_acc_length, non_acc_dim)\n",
    "\n",
    "\n",
    "        batch_size = acc.shape[0]\n",
    "        hidden = model.init_hidden(batch_size).to(device)\n",
    "        cell = model.init_hidden(batch_size).to(device)\n",
    "\n",
    "        lstm_in = torch.cat([a, b], dim = 2)    # (lstm_seq_len, batch, C_cnn + D_nonacc)\n",
    "        # without batch_first = true, LSTM input shape is (seq_len, batch_size, features)\n",
    "\n",
    "#             [DEBUG] ACC CNN output shape: torch.Size([8, 16, 120])\n",
    "# [DEBUG] NON_ACC input shape: torch.Size([8, 120, 2])\n",
    "# [DEBUG] LSTM input shape: torch.Size([120, 8, 18])\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] LSTM input shape: {lstm_in.shape}\")\n",
    "    \n",
    "        # 4) LSTM + classifier\n",
    "        output,(hidden, cell) = self.lstm(lstm_in, (hidden, cell))\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] LSTM output shape: {output.shape}\")\n",
    "\n",
    "        \n",
    "        pre_output = self.hidden2(output)\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] hidden2 shape: {pre_output.shape}\")\n",
    "        y_hat = self.classifier(pre_output)     # (lstm_seq_len, batch, num_sleep_stages)\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] Classifier output shape: {y_hat.shape}\")\n",
    "        return y_hat\n",
    "    def init_hidden(self, batch_size):\n",
    "        return Variable(torch.randn(1,batch_size,self.hidden_size))\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        acc, non_acc, labels = batch\n",
    "        '''\n",
    "        non_acc: (batch, non_acc_length, non_acc_dim)\n",
    "        acc:     (batch, acc_length)  or  (batch, acc_length, 1)\n",
    "        labels:  (batch, non_acc_length)\n",
    "        '''\n",
    "        y_hat = self(non_acc, acc)            # (lstm_seq_len, batch, num_sleep_stages)\n",
    "        y_hat = y_hat.permute(1, 0, 2)        # (batch, lstm_seq_len, num_sleep_stages)\n",
    "    \n",
    "    \n",
    "        # flatten\n",
    "        batch_size, output_length, num_sleep_stages = y_hat.shape\n",
    "        y_hat_flat = y_hat.reshape(batch_size * output_length, num_sleep_stages)\n",
    "        labels_flat  = labels.reshape(batch_size * output_length)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = self.criterion(y_hat_flat, labels_flat)\n",
    "    \n",
    "        self.log('train_loss', loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        acc, non_acc, labels = batch\n",
    "        '''\n",
    "        non_acc: (batch, non_acc_length, non_acc_dim)\n",
    "        acc:     (batch, acc_length)  or  (batch, acc_length, 1)\n",
    "        labels:  (batch, non_acc_length)\n",
    "        '''\n",
    "        y_hat = self(non_acc, acc)            # (lstm_seq_len, batch, num_sleep_stages)\n",
    "        y_hat = y_hat.permute(1, 0, 2)        # (batch, lstm_seq_len, num_sleep_stages)\n",
    "    \n",
    "        # flatten\n",
    "        batch_size, output_length, num_sleep_stages = y_hat.shape\n",
    "        y_hat_flat = y_hat.reshape(batch_size * output_length, num_sleep_stages)\n",
    "        labels_flat  = labels.reshape(batch_size * output_length)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = self.criterion(y_hat_flat, labels_flat)\n",
    "    \n",
    "        # calculate accuracy\n",
    "        predictions = torch.argmax(y_hat_flat, dim=1)\n",
    "        mask = labels_flat != -1\n",
    "        masked_preds = predictions[mask]\n",
    "        masked_labels = labels_flat[mask]\n",
    "        if masked_labels.numel() > 0:\n",
    "            acc = (masked_preds == masked_labels).float().mean().item()\n",
    "        else:\n",
    "            acc = 0.0\n",
    "    \n",
    "        # calculate cohen's kappa\n",
    "        mask = labels_flat != -1\n",
    "        y_valid = labels_flat[mask]\n",
    "        preds_valid = predictions[mask]\n",
    "        if y_valid.numel() > 0:\n",
    "            ckappa = self.kappa(preds_valid, y_valid)\n",
    "        else:\n",
    "            ckappa = torch.tensor(0.0, device=self.device)\n",
    "    \n",
    "        # log metrics\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\",  acc,  prog_bar=True)\n",
    "        self.log(\"val_cohen_kappa\", ckappa, prog_bar=True)\n",
    "    \n",
    "        self.kappa.reset()\n",
    "    \n",
    "        return {\"val_loss\": loss, \"val_acc\": acc}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4787d2b-a143-483b-a67d-b3b1bbc3db4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SleepChunkDataset(subjects_list=[\"S003\"],\n",
    "                                 data_dir=data_dir,\n",
    "                                 debug=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, num_workers=1, shuffle=True)\n",
    "#just a quick lil pass to see if all the shapes are lining up \n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e939a0f-ab60-43a2-939a-062e0086b8f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SleepStager(\n",
       "  (acc_cnn): DeepACTINeT(\n",
       "    (conv1): Conv1d(3, 16, kernel_size=(512,), stride=(2,))\n",
       "    (conv2): Conv1d(16, 16, kernel_size=(256,), stride=(2,))\n",
       "    (conv3): Conv1d(16, 16, kernel_size=(256,), stride=(2,))\n",
       "    (conv4): Conv1d(16, 16, kernel_size=(32,), stride=(2,))\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (batch_norm1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (batch_norm2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (batch_norm3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (batch_norm4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (embeddingLayer): Linear(in_features=2, out_features=16, bias=True)\n",
       "  (lstm): LSTM(32, 64, batch_first=True, bidirectional=True)\n",
       "  (hidden2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (classifier): Linear(in_features=64, out_features=5, bias=True)\n",
       "  (criterion): CrossEntropyLoss()\n",
       "  (kappa): MulticlassCohenKappa()\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#THIS IS WHAT UR TRYING TO RUN\n",
    "\n",
    "model = SleepStager(\n",
    "    cnn_output_channels=16,\n",
    "    lstm_hidden_size=64,\n",
    "    lstm_layers=2,\n",
    "    num_sleep_stages=5,\n",
    "    lr=1e-3,\n",
    "    debug=True\n",
    ")\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3017a816-de5f-41d2-a92e-50a24e26cb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([120])\n",
      "120\n",
      "start shape torch.Size([8, 3, 19200])\n",
      "Input shape after conv1: torch.Size([8, 16, 9345])\n",
      "Input shape after, dropout: torch.Size([8, 16, 9345])\n",
      "Input shape after conv1,dropout,batchnorm: torch.Size([8, 16, 9345])\n",
      "Input shape after maxpool: torch.Size([8, 16, 4672])\n",
      "Input shape after conv2: torch.Size([8, 16, 2209])\n",
      "Input shape after conv3: torch.Size([8, 16, 977])\n",
      "Input shape after maxpool: torch.Size([8, 16, 488])\n",
      "before avg pooling shape: torch.Size([8, 16, 229])\n",
      "Final output shape: torch.Size([8, 16, 120])\n",
      "[DEBUG] ACC CNN output shape: torch.Size([8, 16, 120])\n",
      "[DEBUG] NON_ACC input shape: torch.Size([8, 120, 2])\n",
      "[DEBUG] ACC CNN reshaped shape: torch.Size([120, 8, 16])\n",
      "[DEBUG] NON_ACC reshaped shape: torch.Size([120, 8, 16])\n",
      "[DEBUG] LSTM input shape: torch.Size([120, 8, 32])\n",
      "[DEBUG] LSTM output shape: torch.Size([120, 8, 128])\n",
      "[DEBUG] hidden2 shape: torch.Size([120, 8, 64])\n",
      "[DEBUG] Classifier output shape: torch.Size([120, 8, 5])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    print(targets[0].shape)\n",
    "    outputs = model(data['acc_data'], data['tempbvp_data'])\n",
    "        # outputs: [120, 8, 5] → [8, 120, 5]\n",
    "    outputs = outputs.permute(1, 0, 2)\n",
    "    \n",
    "    # Flatten: [8, 120, 5] → [960, 5]\n",
    "    outputs = outputs.reshape(-1, outputs.shape[-1])\n",
    "    \n",
    "    # targets: [8, 120] → [960]\n",
    "    targets = targets.reshape(-1)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "871df60c-e225-4708-a6ba-e0230180d804",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([120, 5])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba09cc7-367c-48a1-abf2-e2b824e830ca",
   "metadata": {},
   "source": [
    "### REference code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaca7662-5f2e-426b-bbcf-3e80d1a96c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharGRULSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, hidden_size2, output_size, seq_len, rnn_type = \"lstm\", bidirectional=True):\n",
    "        super(CharGRULSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_size2 = hidden_size2\n",
    "        self.output_size = output_size\n",
    "        self.seq_len=seq_len\n",
    "        self.rnntype = rnn_type\n",
    "        self.embeddingLayer = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        if rnn_type == \"gru\":\n",
    "            self.rnn = nn.GRU(\n",
    "                self.hidden_size,\n",
    "                self.hidden_size,\n",
    "                batch_first=True,\n",
    "                bidirectional=self.bidirectional\n",
    "            )\n",
    "        else:\n",
    "            self.rnn = nn.LSTM(\n",
    "                self.hidden_size,\n",
    "                self.hidden_size,\n",
    "                bias=True,\n",
    "                batch_first=True,\n",
    "                bidirectional=self.bidirectional\n",
    "            )\n",
    "            #self.rnn = nn.LSTMCell(self.hidden_size, self.hidden_size, bias=True)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.hidden2 = nn.Linear(self.hidden_size * self.num_directions, self.hidden_size2)\n",
    "        # Only take the output from the final timestep\n",
    "        self.outputLayer = nn.Linear(self.hidden_size2, self.output_size)\n",
    "        \n",
    "        # Can pass on the entirety of lstm_out to the next layer if it is a seq2seq prediction\n",
    "        #self.outputLayer = nn.Linear(self.hidden_size*self.seq_len, self.output_size)\n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.predloss = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    def forward(self, batch, hidden, cell=None):\n",
    "        batchemb = self.embeddingLayer(batch)\n",
    "        batchemb = self.dropout(batchemb)\n",
    "        if self.rnntype == \"lstm\":\n",
    "            #output,(hidden, cell) = self.rnn(batchemb.view(batchemb.shape[0],-1,self.hidden_size), (hidden, cell))\n",
    "            output,(hidden, cell) = self.rnn(batchemb, (hidden, cell))\n",
    "            #hidden, cell = self.rnn(batchemb, (hidden, cell))\n",
    "        else:\n",
    "            hidden = self.rnn(batchemb, hidden)\n",
    "        final_hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)  # [batch_size, hidden*2]\n",
    "        output = self.hidden2(final_hidden)\n",
    "        # Can pass on the entirety of lstm_out to the next layer if it is a seq2seq prediction\n",
    "        #output = self.outputLayer(self.dropout(output.view(-1,self.seq_len*self.hidden_size)))\n",
    "        output=self.outputLayer(output)\n",
    "        output = self.softmax(output)\n",
    "        if self.rnntype == \"lstm\":\n",
    "            return output, hidden, cell\n",
    "        else:\n",
    "            return output, hidden\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        #return Variable(torch.randn(1, self.hidden_size))\n",
    "        num_dir = self.num_directions\n",
    "        return Variable(torch.randn(num_dir,batch_size,self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd09184-000b-45a0-9166-b0cc9434b672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddefe792-f6e7-4156-9439-96770af1bf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    names1=['left_temp',\n",
    "       'right_temp',  'from_19_min', \n",
    "       'log_left_ENMO', 'log_right_ENMO',\n",
    "       'log_left_ENMO_var', 'log_right_ENMO_var',\n",
    "       'log_left_anglex_var', 'log_right_anglex_var', \n",
    "       'log_left_angley_var', 'log_right_angley_var',\n",
    "       'log_left_anglez_var', 'log_right_anglez_var', \n",
    "       'left_anglex', 'left_angley', 'left_anglez',\n",
    "       'right_anglex', 'right_angley', 'right_anglez',\n",
    "       \"log_left_ENMO_range\",\"log_right_ENMO_range\",\n",
    "       \"log_left_anglex_range\",\"log_right_anglex_range\",\n",
    "       \"log_left_angley_range\",\"log_right_angley_range\",\n",
    "       \"log_left_anglez_range\",\"log_right_anglez_range\"]\n",
    "    names2=['participant_id','Disorder', 'sex', 'age','y']\n",
    "    nums=[1,2,14,17,21,23,27,28,29,31,32,34,35,38,39,42,45,48,49,50,51,52,53,56,57,59,60]\n",
    "    data_normalized=pd.DataFrame()\n",
    "    others=pd.DataFrame()\n",
    "    for num in nums:\n",
    "        tmp=data[names1][data[\"participant_id\"]==num]\n",
    "        tmp=(tmp-tmp.mean())/tmp.std()\n",
    "        data_normalized=pd.concat([data_normalized, tmp],axis=0,ignore_index=True)\n",
    "        others_tmp=data[names2][data[\"participant_id\"]==num]\n",
    "        others=pd.concat([others, others_tmp],axis=0,ignore_index=True)\n",
    "    data_normalized=pd.concat([others,data_normalized],axis=1)\n",
    "    return data_normalized\n",
    "\n",
    "# get expanded input\n",
    "def get_LSTM_input(data, nums, back=0, front=0,level=2):\n",
    "    names=['left_temp','right_temp', 'Disorder', 'sex', \n",
    "        'age', 'from_19_min', \n",
    "       'log_left_ENMO_var', 'log_right_ENMO_var',\n",
    "       'log_left_anglex_var', 'log_right_anglex_var', \n",
    "        'log_left_angley_var', 'log_right_angley_var',\n",
    "       'log_left_anglez_var', 'log_right_anglez_var', \n",
    "       'left_anglex', 'left_angley', 'left_anglez',\n",
    "       'right_anglex', 'right_angley', 'right_anglez',\n",
    "       'log_left_ENMO', 'log_right_ENMO',\"log_left_ENMO_range\",\n",
    "       \"log_right_ENMO_range\",\"log_left_anglex_range\",\n",
    "       \"log_right_anglex_range\",\"log_left_angley_range\",\n",
    "       \"log_right_angley_range\",\"log_left_anglez_range\",\n",
    "       \"log_right_anglez_range\"]\n",
    "    #nums=[1,2,14,17,21,23,27,28,29,31,32,34,35,38,39,42,45,48,49,50,51,52,53,56,57,59,60] \n",
    "    X_list=[]\n",
    "    y_list=[]\n",
    "    for num in nums:\n",
    "        #X\n",
    "        sub=data[data[\"participant_id\"]==num]\n",
    "        X=sub[names].values.astype(float)\n",
    "        X=np.reshape(X,(-1,6,len(names)))\n",
    "        n=X.shape[0]\n",
    "        # get original X\n",
    "        exp_X=X[back:n-front,:,:]\n",
    "\n",
    "        # Add back\n",
    "        for i in range(1,back+1):\n",
    "            back_tmp=X[back-i:n-front-i,:,:]\n",
    "            exp_X=np.concatenate((back_tmp,exp_X),axis=1)\n",
    "        #print(ret_X.shape)\n",
    "\n",
    "        # Add Front\n",
    "        for i in range(1,front+1):\n",
    "            front_tmp=X[back+i:n-front+i,:,:]\n",
    "            exp_X=np.concatenate((exp_X, front_tmp),axis=1)\n",
    "        #print(ret_X.shape)\n",
    "        X_list.append(exp_X)\n",
    "        # Y\n",
    "        if level==2:\n",
    "            y=sub[[\"b_y\"]].values.astype(float)\n",
    "        elif level==3:\n",
    "            y=sub[[\"t_y\"]].values.astype(float)\n",
    "        y=y[0::6]\n",
    "        y=np.array([i[0] for i in y])\n",
    "        y=y[back:n-front]\n",
    "        y_list.append(y)\n",
    "    \n",
    "    ret_X=np.concatenate(X_list,axis=0)\n",
    "    ret_y=np.concatenate(y_list,axis=0)\n",
    "    return ret_X,ret_y\n",
    "\n",
    "\n",
    "########### Functions and packages for LSTM ################\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.len = len(X)           \n",
    "        if torch.cuda.is_available():\n",
    "            self.x_data = torch.from_numpy(X).float().cuda()\n",
    "            self.y_data = torch.from_numpy(y).long().cuda()\n",
    "        else:\n",
    "            self.x_data = torch.from_numpy(X).float()\n",
    "            self.y_data = torch.from_numpy(y).long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_data[idx], self.y_data[idx]\n",
    "\n",
    "def calculateAccuracy(y_pred, y):\n",
    "    #y_pred and y are lists\n",
    "    tmp=[1 if y_pred[i]==y[i] else 0 for i in range(len(y))]\n",
    "    return sum(tmp)/len(y)\n",
    "\"\"\"\n",
    "def predict(model, X):\n",
    "    # X in shape #samples*length of seq*#features, all on device\n",
    "    preds=torch.torch.FloatTensor([]).to(device)\n",
    "    hidden = model.init_hidden(batch_size=X.shape[0]).to(device)\n",
    "    cell = model.init_hidden(batch_size=X.shape[0]).to(device)\n",
    "    pred_y, hidden,cell = model(X, hidden,cell)    \n",
    "    preds=torch.cat((preds,pred_y),0)\n",
    "    _, top_i =  preds.data.topk(1)\n",
    "    return top_i\n",
    "\"\"\"\n",
    "def predict(model, val_loader):\n",
    "    # X in shape #samples*length of seq*#features, all on device\n",
    "    preds=torch.torch.FloatTensor([]).to(device)\n",
    "    for i, (records, labels) in enumerate(val_loader): \n",
    "        hidden = model.init_hidden(batch_size=records.shape[0]).to(device)\n",
    "        cell = model.init_hidden(batch_size=records.shape[0]).to(device)\n",
    "        pred_y, hidden,cell = model(records, hidden,cell)    \n",
    "        preds=torch.cat((preds,pred_y),0)\n",
    "    _, top_i =  preds.data.topk(1)\n",
    "    return top_i,preds.data\n",
    "\n",
    "def train(model, train_loader, validation_loader, loss_function, optimizer,num_epochs):\n",
    "    total_step = len(train_loader)\n",
    "    acc_train=[]\n",
    "    loss_train=[]\n",
    "    acc_val=[]\n",
    "    #X_train=train_loader.dataset.x_data.to(device)\n",
    "    #y_train=train_loader.dataset.y_data.to(device)\n",
    "    # This took too much RAM!\n",
    "    #X_val=validation_loader.dataset.x_data.to(device)\n",
    "    y_val=validation_loader.dataset.y_data.to(device)\n",
    "    #Initial hidden\n",
    "    for epoch in range(num_epochs):\n",
    "        y_pred_train=torch.torch.FloatTensor([]).to(device)\n",
    "        y_train=torch.torch.LongTensor([]).to(device)\n",
    "        for i, (records, labels) in enumerate(train_loader): \n",
    "            optimizer.zero_grad()\n",
    "            hidden = model.init_hidden(batch_size=records.shape[0]).to(device)\n",
    "            cell = model.init_hidden(batch_size=records.shape[0]).to(device)\n",
    "            # Move tensors to the configured device\n",
    "            records = records.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Forward pass with data in 30s\n",
    "            pred_y, hidden,cell = model(records, hidden,cell)            \n",
    "            loss = loss_function(pred_y, labels)\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            y_pred_train=torch.cat((y_pred_train,pred_y),0)\n",
    "            y_train=torch.cat((y_train,labels),0)\n",
    "         \n",
    "            if (i+1) % 100 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "        #y_pred_train=model(X_train) #Cost too much RAM!\n",
    "        _, top_i_train =  y_pred_train.data.topk(1)\n",
    "        acc_train_tmp=calculateAccuracy(top_i_train[:,0].tolist(), y_train.tolist())\n",
    "        acc_train.append(acc_train_tmp)\n",
    "        loss_epoch=loss_function(y_pred_train, y_train).item()\n",
    "        loss_train.append(loss_epoch)\n",
    "        #-----------\n",
    "        top_i_val, _ =predict(model, validation_loader)\n",
    "        #-----------\n",
    "        acc_val_tmp=calculateAccuracy(top_i_val[:,0].tolist(), y_val.tolist())\n",
    "        acc_val.append(acc_val_tmp)\n",
    "        \n",
    "    return model, np.array(acc_train),np.array(loss_train), np.array(acc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bd1a48-b06b-495b-aa56-f1efa80e2e13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
