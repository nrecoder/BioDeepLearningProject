{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5b831c-0b5d-4cc6-97e9-baff5d9b0115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import math\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score,precision_score, recall_score, f1_score\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaca7662-5f2e-426b-bbcf-3e80d1a96c78",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (3821355093.py, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 24\u001b[0;36m\u001b[0m\n\u001b[0;31m    bias=True\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "class CharGRULSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, hidden_size2, output_size, seq_len, rnn_type = \"lstm\", bidirectional=True):\n",
    "        super(CharGRULSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_size2 = hidden_size2\n",
    "        self.output_size = output_size\n",
    "        self.seq_len=seq_len\n",
    "        self.rnntype = rnn_type\n",
    "        self.embeddingLayer = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        if rnn_type == \"gru\":\n",
    "            self.rnn = nn.GRU(\n",
    "                self.hidden_size,\n",
    "                self.hidden_size,\n",
    "                batch_first=True,\n",
    "                bidirectional=self.bidirectional\n",
    "            )\n",
    "        else:\n",
    "            self.rnn = nn.LSTM(\n",
    "                self.hidden_size,\n",
    "                self.hidden_size,\n",
    "                bias=True,\n",
    "                batch_first=True,\n",
    "                bidirectional=self.bidirectional\n",
    "            )\n",
    "        #AM HERE IN CONVERSION CHECK CHAT GPT HISTORY FOR CONTINUATION\n",
    "            #self.rnn = nn.LSTMCell(self.hidden_size, self.hidden_size, bias=True)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.hidden2 = nn.Linear(self.hidden_size * self.num_directions, self.hidden_size2)\n",
    "        # Only take the output from the final timestep\n",
    "        self.outputLayer = nn.Linear(self.hidden_size2, self.output_size)\n",
    "        \n",
    "        # Can pass on the entirety of lstm_out to the next layer if it is a seq2seq prediction\n",
    "        #self.outputLayer = nn.Linear(self.hidden_size*self.seq_len, self.output_size)\n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.predloss = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    def forward(self, batch, hidden, cell=None):\n",
    "        batchemb = self.embeddingLayer(batch)\n",
    "        batchemb = self.dropout(batchemb)\n",
    "        if self.rnntype == \"lstm\":\n",
    "            #output,(hidden, cell) = self.rnn(batchemb.view(batchemb.shape[0],-1,self.hidden_size), (hidden, cell))\n",
    "            output,(hidden, cell) = self.rnn(batchemb, (hidden, cell))\n",
    "            #hidden, cell = self.rnn(batchemb, (hidden, cell))\n",
    "        else:\n",
    "            hidden = self.rnn(batchemb, hidden)\n",
    "        final_hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)  # [batch_size, hidden*2]\n",
    "        output = self.hidden2(final_hidden)\n",
    "        # Can pass on the entirety of lstm_out to the next layer if it is a seq2seq prediction\n",
    "        #output = self.outputLayer(self.dropout(output.view(-1,self.seq_len*self.hidden_size)))\n",
    "        output=self.outputLayer(output)\n",
    "        output = self.softmax(output)\n",
    "        if self.rnntype == \"lstm\":\n",
    "            return output, hidden, cell\n",
    "        else:\n",
    "            return output, hidden\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        #return Variable(torch.randn(1, self.hidden_size))\n",
    "        num_dir = self.num_directions\n",
    "        return Variable(torch.randn(num_dir,batch_size,self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddefe792-f6e7-4156-9439-96770af1bf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    names1=['left_temp',\n",
    "       'right_temp',  'from_19_min', \n",
    "       'log_left_ENMO', 'log_right_ENMO',\n",
    "       'log_left_ENMO_var', 'log_right_ENMO_var',\n",
    "       'log_left_anglex_var', 'log_right_anglex_var', \n",
    "       'log_left_angley_var', 'log_right_angley_var',\n",
    "       'log_left_anglez_var', 'log_right_anglez_var', \n",
    "       'left_anglex', 'left_angley', 'left_anglez',\n",
    "       'right_anglex', 'right_angley', 'right_anglez',\n",
    "       \"log_left_ENMO_range\",\"log_right_ENMO_range\",\n",
    "       \"log_left_anglex_range\",\"log_right_anglex_range\",\n",
    "       \"log_left_angley_range\",\"log_right_angley_range\",\n",
    "       \"log_left_anglez_range\",\"log_right_anglez_range\"]\n",
    "    names2=['participant_id','Disorder', 'sex', 'age','y']\n",
    "    nums=[1,2,14,17,21,23,27,28,29,31,32,34,35,38,39,42,45,48,49,50,51,52,53,56,57,59,60]\n",
    "    data_normalized=pd.DataFrame()\n",
    "    others=pd.DataFrame()\n",
    "    for num in nums:\n",
    "        tmp=data[names1][data[\"participant_id\"]==num]\n",
    "        tmp=(tmp-tmp.mean())/tmp.std()\n",
    "        data_normalized=pd.concat([data_normalized, tmp],axis=0,ignore_index=True)\n",
    "        others_tmp=data[names2][data[\"participant_id\"]==num]\n",
    "        others=pd.concat([others, others_tmp],axis=0,ignore_index=True)\n",
    "    data_normalized=pd.concat([others,data_normalized],axis=1)\n",
    "    return data_normalized\n",
    "\n",
    "# get expanded input\n",
    "def get_LSTM_input(data, nums, back=0, front=0,level=2):\n",
    "    names=['left_temp','right_temp', 'Disorder', 'sex', \n",
    "        'age', 'from_19_min', \n",
    "       'log_left_ENMO_var', 'log_right_ENMO_var',\n",
    "       'log_left_anglex_var', 'log_right_anglex_var', \n",
    "        'log_left_angley_var', 'log_right_angley_var',\n",
    "       'log_left_anglez_var', 'log_right_anglez_var', \n",
    "       'left_anglex', 'left_angley', 'left_anglez',\n",
    "       'right_anglex', 'right_angley', 'right_anglez',\n",
    "       'log_left_ENMO', 'log_right_ENMO',\"log_left_ENMO_range\",\n",
    "       \"log_right_ENMO_range\",\"log_left_anglex_range\",\n",
    "       \"log_right_anglex_range\",\"log_left_angley_range\",\n",
    "       \"log_right_angley_range\",\"log_left_anglez_range\",\n",
    "       \"log_right_anglez_range\"]\n",
    "    #nums=[1,2,14,17,21,23,27,28,29,31,32,34,35,38,39,42,45,48,49,50,51,52,53,56,57,59,60] \n",
    "    X_list=[]\n",
    "    y_list=[]\n",
    "    for num in nums:\n",
    "        #X\n",
    "        sub=data[data[\"participant_id\"]==num]\n",
    "        X=sub[names].values.astype(float)\n",
    "        X=np.reshape(X,(-1,6,len(names)))\n",
    "        n=X.shape[0]\n",
    "        # get original X\n",
    "        exp_X=X[back:n-front,:,:]\n",
    "\n",
    "        # Add back\n",
    "        for i in range(1,back+1):\n",
    "            back_tmp=X[back-i:n-front-i,:,:]\n",
    "            exp_X=np.concatenate((back_tmp,exp_X),axis=1)\n",
    "        #print(ret_X.shape)\n",
    "\n",
    "        # Add Front\n",
    "        for i in range(1,front+1):\n",
    "            front_tmp=X[back+i:n-front+i,:,:]\n",
    "            exp_X=np.concatenate((exp_X, front_tmp),axis=1)\n",
    "        #print(ret_X.shape)\n",
    "        X_list.append(exp_X)\n",
    "        # Y\n",
    "        if level==2:\n",
    "            y=sub[[\"b_y\"]].values.astype(float)\n",
    "        elif level==3:\n",
    "            y=sub[[\"t_y\"]].values.astype(float)\n",
    "        y=y[0::6]\n",
    "        y=np.array([i[0] for i in y])\n",
    "        y=y[back:n-front]\n",
    "        y_list.append(y)\n",
    "    \n",
    "    ret_X=np.concatenate(X_list,axis=0)\n",
    "    ret_y=np.concatenate(y_list,axis=0)\n",
    "    return ret_X,ret_y\n",
    "\n",
    "\n",
    "########### Functions and packages for LSTM ################\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.len = len(X)           \n",
    "        if torch.cuda.is_available():\n",
    "            self.x_data = torch.from_numpy(X).float().cuda()\n",
    "            self.y_data = torch.from_numpy(y).long().cuda()\n",
    "        else:\n",
    "            self.x_data = torch.from_numpy(X).float()\n",
    "            self.y_data = torch.from_numpy(y).long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_data[idx], self.y_data[idx]\n",
    "\n",
    "def calculateAccuracy(y_pred, y):\n",
    "    #y_pred and y are lists\n",
    "    tmp=[1 if y_pred[i]==y[i] else 0 for i in range(len(y))]\n",
    "    return sum(tmp)/len(y)\n",
    "\"\"\"\n",
    "def predict(model, X):\n",
    "    # X in shape #samples*length of seq*#features, all on device\n",
    "    preds=torch.torch.FloatTensor([]).to(device)\n",
    "    hidden = model.init_hidden(batch_size=X.shape[0]).to(device)\n",
    "    cell = model.init_hidden(batch_size=X.shape[0]).to(device)\n",
    "    pred_y, hidden,cell = model(X, hidden,cell)    \n",
    "    preds=torch.cat((preds,pred_y),0)\n",
    "    _, top_i =  preds.data.topk(1)\n",
    "    return top_i\n",
    "\"\"\"\n",
    "def predict(model, val_loader):\n",
    "    # X in shape #samples*length of seq*#features, all on device\n",
    "    preds=torch.torch.FloatTensor([]).to(device)\n",
    "    for i, (records, labels) in enumerate(val_loader): \n",
    "        hidden = model.init_hidden(batch_size=records.shape[0]).to(device)\n",
    "        cell = model.init_hidden(batch_size=records.shape[0]).to(device)\n",
    "        pred_y, hidden,cell = model(records, hidden,cell)    \n",
    "        preds=torch.cat((preds,pred_y),0)\n",
    "    _, top_i =  preds.data.topk(1)\n",
    "    return top_i,preds.data\n",
    "\n",
    "def train(model, train_loader, validation_loader, loss_function, optimizer,num_epochs):\n",
    "    total_step = len(train_loader)\n",
    "    acc_train=[]\n",
    "    loss_train=[]\n",
    "    acc_val=[]\n",
    "    #X_train=train_loader.dataset.x_data.to(device)\n",
    "    #y_train=train_loader.dataset.y_data.to(device)\n",
    "    # This took too much RAM!\n",
    "    #X_val=validation_loader.dataset.x_data.to(device)\n",
    "    y_val=validation_loader.dataset.y_data.to(device)\n",
    "    #Initial hidden\n",
    "    for epoch in range(num_epochs):\n",
    "        y_pred_train=torch.torch.FloatTensor([]).to(device)\n",
    "        y_train=torch.torch.LongTensor([]).to(device)\n",
    "        for i, (records, labels) in enumerate(train_loader): \n",
    "            optimizer.zero_grad()\n",
    "            hidden = model.init_hidden(batch_size=records.shape[0]).to(device)\n",
    "            cell = model.init_hidden(batch_size=records.shape[0]).to(device)\n",
    "            # Move tensors to the configured device\n",
    "            records = records.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Forward pass with data in 30s\n",
    "            pred_y, hidden,cell = model(records, hidden,cell)            \n",
    "            loss = loss_function(pred_y, labels)\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            y_pred_train=torch.cat((y_pred_train,pred_y),0)\n",
    "            y_train=torch.cat((y_train,labels),0)\n",
    "         \n",
    "            if (i+1) % 100 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "        #y_pred_train=model(X_train) #Cost too much RAM!\n",
    "        _, top_i_train =  y_pred_train.data.topk(1)\n",
    "        acc_train_tmp=calculateAccuracy(top_i_train[:,0].tolist(), y_train.tolist())\n",
    "        acc_train.append(acc_train_tmp)\n",
    "        loss_epoch=loss_function(y_pred_train, y_train).item()\n",
    "        loss_train.append(loss_epoch)\n",
    "        #-----------\n",
    "        top_i_val, _ =predict(model, validation_loader)\n",
    "        #-----------\n",
    "        acc_val_tmp=calculateAccuracy(top_i_val[:,0].tolist(), y_val.tolist())\n",
    "        acc_val.append(acc_val_tmp)\n",
    "        \n",
    "    return model, np.array(acc_train),np.array(loss_train), np.array(acc_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
